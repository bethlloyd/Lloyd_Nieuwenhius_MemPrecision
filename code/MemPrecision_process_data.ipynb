{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "from scipy.stats import linregress, sem\n",
    "from scipy.signal import chirp, find_peaks, peak_widths, butter\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "from netneurotools import stats as st\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy import stats\n",
    "libfolder=os.getcwd()\n",
    "sys.path.append(libfolder)\n",
    "from mem_rew_stimfuncs import *\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings: can all be adjusted, see description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrbp_path = 'C:\\\\Users\\\\lloydb\\\\surfdrive\\\\ExperimentData\\\\HRBP_MP'  # change this to own path\n",
    "\n",
    "# settings \n",
    "minimum_distance_centre = 150  # this is a check done during the test --> make sure pps placed the item on/close to the circle, items just clicked as final location in centre screen = invalid response. \n",
    "sF = 40                        # pupil sample frequency \n",
    "set_invalid_threshold = 50.0   # % of invalid samples in event \n",
    "\n",
    "# Start and end event times\n",
    "baseline_dur = 0.2              # duration of pre-item event\n",
    "stim_win_end = 6.0              # epoch time to inspect timeseries during study phase: stimulus\n",
    "fb_win_end=3.0                  # epoch time to inspect timeseries during study phase: feedback\n",
    "stim_dur = 3                    # seconds the stimuli is in its location during encoding\n",
    "duration_peak = 0.2             # within the two event timeseries, this is the duration of the mean around the peak pupil response (SP_peak_response and FB_peak_response)\n",
    "duration_max_anticipation = 0.2 # this is pupil change measure during the anticipation event \n",
    "other_event_dur = 0.2\n",
    "resp_onset_check = 6.0\n",
    "# missing data: (top one not needed in script because it runs through only data that's available)\n",
    "#total_missing_runs = {'003': [6], '023': [6], '034': [1], '035': [4], '037': [5], \n",
    "#                '038': [5,6], '040': [1], '041': [5,6], '047': [3], '049': [4,6], '051': [4]}\n",
    "# missing data: \n",
    "pup_missing_runs = {'003': ['6'], '035': ['4'], '037': ['5'],  '049': ['4','6'], '051': ['4']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These are some handy quick functions that are used in the larger functions below. \n",
    "Should be self-explanatory \n",
    "\n",
    "'''\n",
    "\n",
    "def get_behav_paths(sub):\n",
    "    \n",
    "    HRBP_data = {'study_data': glob.glob('{}/raw_data/data/HRBP_MP{}/run*_HRBP_MP_phase1*.xpd'.format(hrbp_path,sub)),\n",
    "                 'test_data': glob.glob('{}/raw_data/data/HRBP_MP{}/run*_HRBP_MP_phase2*.xpd'.format(hrbp_path,sub)),\n",
    "                 'triallist_path': '{}/triallist_ver2'.format(hrbp_path)}\n",
    "    return HRBP_data\n",
    "\n",
    "def get_pup_paths(sub, run):\n",
    "    HRBP_data =   {'pup_rawdata': glob.glob('{}/raw_data/tobii_data/HRBP_MP{}/PupCor_output/*run{}*_raw_pup*'.format(hrbp_path,sub,run)),\n",
    "                  'pup_smthdata': glob.glob('{}/raw_data/tobii_data/HRBP_MP{}/PupCor_output/*run{}*_expdata_smth_int_pup.txt'.format(hrbp_path,sub,run)),\n",
    "                  'markerfile': glob.glob('{}/raw_data/tobii_data/HRBP_MP{}/study_phase*run{}*_markers.tsv'.format(hrbp_path,sub,run)),\n",
    "                  'tsvdat_file': glob.glob('{}/raw_data/tobii_data/HRBP_MP{}/study_phase*run{}*_expdata.tsv'.format(hrbp_path,sub,run)),\n",
    "                  'FS_corr_pup_smthdata': glob.glob('{}/raw_data/tobii_data/HRBP_MP{}/foreshadow_correction/PupCor_output/*run{}*_expdata_smth_int_pup.txt'.format(hrbp_path,sub,run))}\n",
    "    return HRBP_data\n",
    "\n",
    "def make_empty_array(time_start, time_end, sF):\n",
    "\n",
    "    empty_array = np.empty((1,int((time_end--time_start)*sF)))\n",
    "    empty_array[:] = np.NaN # make array list \n",
    "    empty_array=np.array(empty_array).ravel() # unravel\n",
    "    \n",
    "    return empty_array.tolist()\n",
    "\n",
    "def chop_time_series(data, sF, onset, window_start, window_end):\n",
    "    data_save = data[onset-int(window_start*sF):onset+int(window_end*sF)]\n",
    "    return data_save\n",
    "\n",
    "def chop_time_series_flipped(data, sF, onset, window_start, window_end):\n",
    "    data_save = data[onset+int(window_start*sF):onset-int(window_end*sF)]\n",
    "    return data_save\n",
    "\n",
    "def bl_correct_percChange(data, baseline):\n",
    "    data_corrected = ((data - baseline) / baseline)*100 \n",
    "    \n",
    "    return data_corrected\n",
    "\n",
    "def bl_correct_absolutePupil(data, baseline):\n",
    "    data_corrected = (data - baseline)\n",
    "    \n",
    "    return data_corrected\n",
    "    \n",
    "# define a function to compute z-scores\n",
    "def zscore(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "def process_event_data(raw_pup, smth_pup, sF, event_data, set_invalid_threshold, baseline_method):\n",
    "    raw_trial_dat = chop_time_series(raw_pup, sF, event_data[\"onset\"], event_data[\"win_str\"], event_data[\"win_end\"])\n",
    "    prop_invalid = (sum([int(x==-1) for x in raw_trial_dat])/len(raw_trial_dat))*100  # get proportion of invalid samles in event\n",
    "    \n",
    "    if (prop_invalid < set_invalid_threshold) and (event_data['BL_prop_invalid'] < set_invalid_threshold):\n",
    "        event_data_cut = chop_time_series(smth_pup, sF, event_data[\"onset\"], event_data[\"win_str\"], event_data[\"win_end\"])\n",
    "        event_data_clean = remove_spur_samples(event_data_cut)\n",
    "        \n",
    "        # baseline correct the event \n",
    "        if baseline_method == None:\n",
    "            event_data_BLcorrected = event_data_clean\n",
    "        elif baseline_method == 'percentage':\n",
    "            event_data_BLcorrected = bl_correct_percChange(event_data_clean, event_data[\"baseline\"])\n",
    "        elif baseline_method == 'absolute':\n",
    "            event_data_BLcorrected = bl_correct_absolutePupil(event_data_clean, event_data[\"baseline\"])\n",
    "       \n",
    "    else:\n",
    "        event_data_BLcorrected = make_empty_array(event_data[\"win_str\"], event_data[\"win_end\"], sF)\n",
    "    return event_data_BLcorrected\n",
    "\n",
    "def get_peak_pup_resp(pup_timeseries, duration_peak, sF):\n",
    "\n",
    "    start_point = int(np.argmax(pup_timeseries)-((duration_peak/2)*sF))\n",
    "    end_point = int(np.argmax(pup_timeseries)+((duration_peak/2)*sF))\n",
    "    if start_point < 0:\n",
    "        start_point = 0\n",
    "    if end_point > len(pup_timeseries):\n",
    "        end_point = len(pup_timeseries)\n",
    "    \n",
    "    pup_response = np.mean(pup_timeseries[start_point:end_point])\n",
    "    return pup_response\n",
    "\n",
    "def apply_median_split(df, pupil):\n",
    "    median_size = df[pupil].median()\n",
    "    df[f'{pupil}_median_split'] = df[pupil].apply(lambda x: 'large' if pd.notna(x) and x >= median_size else ('small' if pd.notna(x) else np.nan))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_data(sub, save = True):\n",
    "    \n",
    "    '''\n",
    "    This func loads in the behavioural data:\n",
    "        - extracts information from study and test phase\n",
    "        - runs the distance-correction check on the test responses (if resp>dis then NA in that trial)\n",
    "        - if save = True: saves a file with subject-specific trial-level data \n",
    "        - returns the subject trial data dataframe + some averages \n",
    "    '''\n",
    "    \n",
    "    # get study + test data \n",
    "    data = get_behav_paths(sub)\n",
    "    #print(f'running subject: {sub}')\n",
    "    if len(data['study_data']) < 6: \n",
    "        print(f'missing study runs for subject {sub}')\n",
    "    elif len(data['test_data']) < 6: \n",
    "        print(f'missing test runs for subject {sub}')\n",
    "        \n",
    "    sub_Df = pd.DataFrame()\n",
    "    ave_Df = pd.DataFrame()\n",
    "    prop_rew = []\n",
    "    \n",
    "    # loop over runs\n",
    "    for i, (study_data, test_data) in enumerate(zip(data['study_data'], data['test_data'])):   \n",
    "        \n",
    "        if study_data.split('\\\\')[-1][3] != test_data.split('\\\\')[-1][3]:\n",
    "            raise ValueError(f'study and test data runs are not aligned for subject: {sub}')\n",
    "        else: \n",
    "            run = study_data.split('\\\\')[-1][3]\n",
    "        \n",
    "        # confirm which triallists to take and open files \n",
    "        study_data = [line.rstrip() for line in open(study_data)]\n",
    "        test_data = [line.rstrip() for line in open(test_data)]\n",
    "        study_triallist = extract_tiallist('{}/HRBP_MP{}_phase1_run{}.txt'.format(data['triallist_path'],sub, str(run)))\n",
    "        test_triallist = extract_tiallist('{}/HRBP_MP{}_phase2_run{}.txt'.format(data['triallist_path'],sub, str(run)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # get information from study data\n",
    "        #-----------------------------------------\n",
    "        # picture: picture_study\n",
    "        picture_study = [line[4] for line in study_triallist]\n",
    "        ITI_study = [line[6] for line in study_triallist]\n",
    "        \n",
    "        # location data: dot_resp_loc, cat_corr\n",
    "        dot_resp_loc = [line.split(\",\")[-1] for line in study_data if \"mouse_response\" in line]\n",
    "        time_stamp_dot_resp = [line.split(\",\")[2] for line in study_data if \"mouse_response\" in line]   # taking this timing because of pspm --> want to model mouse response in pupil (but this is the easiest way to get mouse timing!)\n",
    "\n",
    "        all_trials = [line.split(\",\")[1] for line in study_data if \"mouse_response\" in line or \"end_trial\" in line]\n",
    "        check = test_sequentiality(all_trials, 'end_trial', 2)  # check if there is skip (end_trial appears twice !)\n",
    "        if check:\n",
    "            for ind in check:\n",
    "                new_index = round(float((ind+1)/2))\n",
    "                dot_resp_loc.insert(new_index, \"NaN\")\n",
    "                time_stamp_dot_resp.insert(new_index, 'NaN')\n",
    "                \n",
    "        if all_trials[0] == 'end_trial':\n",
    "            dot_resp_loc.insert(0, \"NaN\")\n",
    "            time_stamp_dot_resp.insert(0, 'NaN')\n",
    "            \n",
    "        start_exp_ts= int([line.split(\",\")[2] for line in study_data if \"start_experiment\" in line][0])\n",
    "        corr_time_stamp_dot_resp = []\n",
    "        for i in time_stamp_dot_resp:\n",
    "            if i != 'NaN':\n",
    "                corr_time_stamp = int(i) - start_exp_ts\n",
    "                corr_time_stamp_dot_resp.append(corr_time_stamp)\n",
    "            else:\n",
    "                corr_time_stamp_dot_resp.append('NaN')\n",
    "\n",
    "        target_dot = [line[9] for line in study_triallist]\n",
    "        cat_corr = []\n",
    "        for target, resp in zip(target_dot, dot_resp_loc):\n",
    "            if resp != \"NaN\":\n",
    "                if (rel_error_degrees(int(target), int(resp)) <= 20):\n",
    "                    cat_corr.append(1)\n",
    "                elif (rel_error_degrees(int(target), int(resp)) > 20):\n",
    "                    cat_corr.append(0)\n",
    "            elif resp == \"NaN\":\n",
    "                cat_corr.append(0)\n",
    "            \n",
    "        # TN data: TN_corr, TN_rt\n",
    "        TN_resp = [line.split(\",\")[3] for line in study_data if \"key_resp_TN\" in line]\n",
    "        TN_key = [int(line[-1]) for line in study_triallist]\n",
    "        TN_corr = []\n",
    "        for resp, key in zip(TN_resp, TN_key):\n",
    "            if resp != \"NaN\" and str(key) == resp:\n",
    "                TN_corr.append(1)\n",
    "            elif resp == \"NaN\" or str(key) != resp:\n",
    "                TN_corr.append(0)\n",
    "        TN_rt = [line.split(\",\")[4] for line in study_data if \"key_resp_TN\" in line]\n",
    "        TN_rt = [int(i) if i != \"NaN\" else i for i in TN_rt]\n",
    "        \n",
    "        # feedback: feedback_type\n",
    "        feedback_type = [line.split(\",\")[1] for line in study_data if \"feedback_onset\" in line]\n",
    "      \n",
    "\n",
    "    \n",
    "    \n",
    "        # get information from test data\n",
    "        #-----------------------------------------\n",
    "        # location data: corr_response_pos\n",
    "        \n",
    "        # bug fixing for skipped trial during test! (rarely happens but could skip the precision question randomly)\n",
    "        response_pos = [line.split(\",\")[-1] for line in test_data if \"mouse_response\" in line]\n",
    "        dis_from_centre = [line.split(\",\")[5] for line in test_data if \"mouse_response\" in line]\n",
    "        all_trials = [line.split(\",\")[1] for line in test_data if \"mouse_response\" in line or \"end_trial\" in line]\n",
    "        \n",
    "        # check if there is skip (end_trial appears twice !)\n",
    "        check = test_sequentiality(all_trials, 'end_trial', 2)\n",
    "        if check:\n",
    "            for ind in check:\n",
    "                new_index = round(float((ind+1)/2))\n",
    "                response_pos.insert(new_index, \"NaN\")\n",
    "                dis_from_centre.insert(new_index, \"NaN\")\n",
    "        if all_trials[0] == 'end_trial':\n",
    "            response_pos.insert(0, \"NaN\")\n",
    "            dis_from_centre.insert(0, \"NaN\")\n",
    "        # correct precision response for distance from centre\n",
    "        corr_response_pos = correct_resp_based_on_distance(dis_from_centre, response_pos, minimum_distance_centre)\n",
    "        \n",
    "        # calculate minimum error: target_pos - response_pos \n",
    "        target_pos = [line[5] for line in test_triallist]\n",
    "        minimum_error=[]\n",
    "        for i in range(len(target_pos)):\n",
    "            if target_pos[i] == 'NaN':\n",
    "                minimum_error.append('NaN')\n",
    "            elif corr_response_pos[i] == 'NaN':\n",
    "                minimum_error.append('NaN')\n",
    "            else: \n",
    "                error = rel_error_degrees(int(target_pos[i]), int(corr_response_pos[i]))\n",
    "                minimum_error.append(error)\n",
    "\n",
    "        # Create run dataframe\n",
    "        run_Df = pd.DataFrame({'sub': len(test_triallist) * [sub],\n",
    "                               'run': len(test_triallist) * [run],\n",
    "                               'cat_cond': [int(line[1]) for line in test_triallist],\n",
    "                               'trial_type': [line[3] for line in test_triallist],\n",
    "                               'picture': [line[2] for line in test_triallist],\n",
    "                               'old':[1 if line[4] == 'old' else 0 for line in test_triallist],\n",
    "                               'response_old': [1 if int(line.split(\",\")[3])==0 else 0 for line in test_data if \"old_new_resp\" in line],\n",
    "                               'target_pos':target_pos,\n",
    "                               'response_pos': corr_response_pos,\n",
    "                               'minimum_error': minimum_error})\n",
    "        \n",
    "        # get proportion of trials rewarded: \n",
    "        prop_rew_run = [line.split(\",\")[-1] for line in study_data if \"PROP_REWARDED\" in line]\n",
    "        prop_rew.append(float(prop_rew_run[0]))\n",
    "\n",
    "        # drop new trials from dataframe (not analysing those)\n",
    "        run_Df = run_Df.drop(run_Df[run_Df.old == 0].index)\n",
    "        # reorder the dataframe to study order \n",
    "        run_Df['picture_order'] = picture_study\n",
    "        run_Df[\"indices\"] = run_Df[\"picture\"].map(lambda x: picture_study.index(x))\n",
    "        run_Df = run_Df.sort_values(by=[\"indices\"])\n",
    "        # drop unnecessary columns \n",
    "        run_Df = run_Df.drop(columns=['picture_order', 'indices'])\n",
    "        \n",
    "        # add the columns from the study information (all in order now)\n",
    "        run_Df['dot_resp_loc'] = dot_resp_loc\n",
    "        run_Df['cat_corr'] = cat_corr\n",
    "        run_Df['TN_corr'] = TN_corr\n",
    "        run_Df['TN_rt'] = TN_rt\n",
    "        run_Df['ITI_dur'] =  ITI_study\n",
    "        run_Df['feedback_type'] = feedback_type\n",
    "        run_Df['trial'] = list(range(1, 15))\n",
    "        run_Df['mouse_resp_timestamp'] = corr_time_stamp_dot_resp\n",
    "\n",
    "        # Create a new column 'trial_type_tm1' based on previous row's value\n",
    "        run_Df['trial_type_tm1'] = run_Df['trial_type'].shift(1)\n",
    "        # Assign 'csm' if there is no previous row (i.e., the first index)\n",
    "        run_Df.loc[run_Df.index[0], 'trial_type_tm1'] = 'csm'\n",
    "        \n",
    "        # do the same for ITI \n",
    "        run_Df['ITI_dur_tm1'] =  run_Df['ITI_dur'].shift(1)\n",
    "        # Assign ave ITI if there is no previous row (i.e., the first index)\n",
    "        run_Df.loc[run_Df.index[0], 'ITI_dur_tm1'] = 4500\n",
    "        \n",
    "        sub_Df = sub_Df.append(run_Df, ignore_index=True)\n",
    "        \n",
    "        if save:\n",
    "            save_fn = f'../../stats/1_preprocessed/sub-{sub}/sub-{sub}_precision_data_old.csv'\n",
    "            sub_Df.to_csv(save_fn, index=False)\n",
    "        \n",
    "    # log the proportion of rewarded trials \n",
    "    prop = round(float(sum(prop_rew) / len(prop_rew)),2)\n",
    "    ave_Df = pd.DataFrame({'sub': sub,\n",
    "                           'prop_rewarded': [prop]})\n",
    "\n",
    "    return sub_Df, ave_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pupilData(sub, overwrite = False, foreshortening_correction = True):\n",
    "    \n",
    "    '''\n",
    "    This func first runs the behavioural data function:\n",
    "        - then loads in pupil data (choose foreshortening corrected or not)\n",
    "        - performs: \n",
    "             1. Low-pass filter tor run using a 10Hzfourth-order Butterworth filter\n",
    "             2. Flags trial events with >% of invalid samples    \n",
    "             3. 3 SD thresholding to event (remove spurious samples within event)\n",
    "             4. Chops timeseries into events (with baseline correction if needed)\n",
    "        - adds timeseries and averages as new columns into the behavioural dataframe \n",
    "        - returns this dataframe (can append subs and save later - whateverrr)\n",
    "    '''\n",
    "\n",
    "    behav_data, ave_Df = get_precision_data(sub)\n",
    "\n",
    "    # make new columns in the behav df --> prepare for all the new pupil data :) \n",
    "    behav_data[\"SP_timeseries\"] = [[] for _ in range(behav_data.shape[0])]\n",
    "    behav_data[\"FB_timeseries\"] = [[] for _ in range(behav_data.shape[0])]\n",
    "    \n",
    "    behav_data[\"SP_peak_response\"] = np.nan\n",
    "    behav_data[\"FB_peak_response\"] = np.nan\n",
    "    behav_data[\"preStim_bl\"] = np.nan\n",
    "\n",
    "    behav_data[\"anticipation\"] = np.nan        # pupil change value (final 0.2 - first 0.2 of anticipation)\n",
    "    behav_data[\"start_anticipation\"] = np.nan  # based on first 0.2s of the anticipation ITI (pre-item bl corrected)\n",
    "    behav_data[\"end_anticipation\"] = np.nan    # based on first 0.2s of the anticipation ITI (pre-item bl corrected)\n",
    "    behav_data[\"choice_onset_event\"] = np.nan  # based on the 0.5s after choice onset (pre-item baseline corrected)\n",
    "    behav_data[\"resp_onset_event\"] = [[] for _ in range(behav_data.shape[0])]\n",
    "\n",
    "    invalid_BL = []\n",
    "    invalid_choice = []\n",
    "    invalid_anticipation = []\n",
    "    \n",
    "    for run in set(behav_data['run']):  # only get pupil data for the equivilent behaivoural data (for example, if subject is missing only the behavioural data for a run, then their pupil data is not analysed)\n",
    "\n",
    "        if sub in pup_missing_runs and run in pup_missing_runs[sub]:  # where subjects are missing only the pupil data --> there will be NaNs in the dataframe for all pupil columns\n",
    "            print(f'missing pupil data for run {run}, sub:{sub}, keeping NaNs in df!')\n",
    "\n",
    "        else:\n",
    "            print(f'going on with run: {run}')\n",
    "\n",
    "            all_data = get_pup_paths(sub, run)  # get the pupil data we need\n",
    "\n",
    "            # load pupil data \n",
    "            if foreshortening_correction:   # gaze-corrected and blink interopelated/smoothed          \n",
    "                smth_pup = np.loadtxt(all_data['FS_corr_pup_smthdata'][0])\n",
    "\n",
    "            else:                           # only blink interopelated/smoothed        \n",
    "                smth_pup = np.loadtxt(all_data['pup_smthdata'][0])\n",
    "            raw_pup = np.loadtxt(all_data['pup_rawdata'][0])    \n",
    "            markers = pd.read_csv(all_data['markerfile'][0], delimiter = \"\\t\")\n",
    "            tsvdat = pd.read_csv(all_data['tsvdat_file'][0], delimiter = \"\\t\")\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # 1. Apply  low-pass filter tor run using a 10Hzfourth-order Butterworth filter\n",
    "            #------------------------------------------------------------------------------\n",
    "\n",
    "            bet, ab = signal.butter(4, 10, 'lowpass', fs=40) \n",
    "            smth_pup_filt = signal.filtfilt(bet, ab, smth_pup, padlen=0)\n",
    "\n",
    "            # get the onset samples for each event (stim, TN, FB)\n",
    "            stim_onset = round((markers.query(\"Marker == 'stim_onset'\")['TimeStamp'] - tsvdat['TimeStamp'][0])/1000*sF).astype(int)\n",
    "            resp_onset = list(round((markers.query(\"Marker == 'mouse_response'\")['TimeStamp'] - tsvdat['TimeStamp'][0])/1000*sF).astype(int))\n",
    "            tn_onset = round((markers.query(\"Marker == 'target_num_onset'\")['TimeStamp'] - tsvdat['TimeStamp'][0])/1000*sF).astype(int)\n",
    "            fb_onset = round((markers.query(\"Marker == 'feedback_onset'\")['TimeStamp'] - tsvdat['TimeStamp'][0])/1000*sF).astype(int)\n",
    "\n",
    "            for i, (a,b) in enumerate(zip(stim_onset, tn_onset)):  # resp_onset only looks at trials where a location response was made (check this is in line with behavioural data!)'\n",
    "                if len(resp_onset) <= i:\n",
    "                    resp_onset.append('nan')\n",
    "                elif not a < resp_onset[i] < b:\n",
    "                    resp_onset.insert(i, 'nan')\n",
    "                    \n",
    "            # Loop over trials of 3 events \n",
    "            for trial, (st_on, resp_on, tn_on, fb_on) in enumerate(zip(stim_onset, resp_onset, tn_onset,fb_onset)):\n",
    "                trial = trial+1\n",
    "\n",
    "                 # Baseline stim\n",
    "                raw_trial_dat  = raw_pup[int(st_on)-int(baseline_dur*sF):int(st_on)]  # takes pupil samples 0.5 s before onset \n",
    "                BL_stim_prop_invalid = (sum([int(x==-1) for x in raw_trial_dat])/len(raw_trial_dat))*100\n",
    "                mean_preStim_baseline = mean(remove_spur_samples(smth_pup[int(st_on)-int(baseline_dur*sF):int(st_on)]))  # calculate the average BL (removing spur samples)\n",
    "                \n",
    "                # log proportion of invalid trials BL: \n",
    "                if BL_stim_prop_invalid > set_invalid_threshold:\n",
    "                    invalid_BL.append(1)\n",
    "                else:\n",
    "                    invalid_BL.append(0)\n",
    "                    \n",
    "                # Baseline feedback\n",
    "                raw_trial_dat  = raw_pup[int(fb_on)-int(baseline_dur*sF):int(fb_on)]   # takes pupil samples 0.5 s before onset \n",
    "                BL_fb_prop_invalid = (sum([int(x==-1) for x in raw_trial_dat])/len(raw_trial_dat))*100\n",
    "                mean_preFb_baseline = mean(remove_spur_samples(smth_pup[int(fb_on)-int(baseline_dur*sF):int(fb_on)]))  # calculate the average BL (removing spur samples)\n",
    "\n",
    "                # save the raw basline data for each trial (to look at pupil drift)\n",
    "                index = int(behav_data.index[(behav_data['run'] == run) & (behav_data['trial'] == trial)][0])\n",
    "                behav_data.iat[index, behav_data.columns.get_loc('preStim_bl')] = mean_preStim_baseline\n",
    "\n",
    "\n",
    "                # 2. Flag trial events with >% of invalid samples    (done below)\n",
    "                #----------------------------------------------------------------------\n",
    "\n",
    "                # 3. apply 3 SD thresholding to event (remove spurious samples within event)\n",
    "                #---------------------------------------------------------------------------\n",
    "\n",
    "                events = [\n",
    "                    {\"name\": \"SP_timeseries\", \"peak\": \"SP_peak_response\", \"onset\": st_on, \"win_str\": baseline_dur, \"win_end\": stim_win_end, \"baseline\": mean_preStim_baseline, 'BL_prop_invalid': BL_stim_prop_invalid},\n",
    "                    {\"name\": \"FB_timeseries\", \"peak\": \"FB_peak_response\", \"onset\": fb_on, \"win_str\": baseline_dur, \"win_end\": fb_win_end, \"baseline\": mean_preFb_baseline, 'BL_prop_invalid': BL_fb_prop_invalid},\n",
    "                    {\"name\": \"anticipation\", \"resp_onset\": resp_on, \"tn_onset\": tn_on},\n",
    "                    {\"name\": \"start_anticipation\",\"resp_onset\": resp_on, \"tn_onset\": tn_on, \"baseline\": mean_preStim_baseline},\n",
    "                    {\"name\": \"end_anticipation\", \"resp_onset\": resp_on, \"tn_onset\": tn_on, \"baseline\": mean_preStim_baseline},\n",
    "                    {\"name\": \"choice_onset_event\", \"baseline\": mean_preStim_baseline, 'BL_prop_invalid': BL_stim_prop_invalid},\n",
    "                    {\"name\": \"resp_onset_event\", \"onset\":resp_on, \"win_str\": 0, \"win_end\": 6.0, \"baseline\": mean_preStim_baseline, 'BL_prop_invalid': BL_stim_prop_invalid}\n",
    "                ]\n",
    "\n",
    "                for event in events:\n",
    "                    \n",
    "                    \n",
    "                    # first save the timseries events (not used for analysis - just visualisation)\n",
    "                    if (event['name'] == \"SP_timeseries\") or (event['name'] == \"FB_timeseries\"):\n",
    "                        output = process_event_data(raw_pup, smth_pup, sF, event, set_invalid_threshold, baseline_method = 'absolute')  # <<-- this function cuts out the events, checks if there are too many invalid samples (in both baseline and event), applies the 3SD criteria and returns the 'final' trial event! \n",
    "                         \n",
    "                        # slot in the timeseries to the correct rows in the dataframe\n",
    "                        index = int(behav_data.index[(behav_data['run'] == run) & (behav_data['trial'] == trial)][0]) \n",
    "                        behav_data.iat[index, behav_data.columns.get_loc(event['name'])] = output  # log the timeseries here\n",
    "                        behav_data.iat[index, behav_data.columns.get_loc(event['peak'])] = get_peak_pup_resp(output, duration_peak, sF)  # log the peak pupil response here\n",
    "                   \n",
    "                            \n",
    "                    elif event['name'] == 'resp_onset_event':\n",
    "                        if resp_on == 'nan':\n",
    "                            output = make_empty_array(0, 6.0, sF)   # log nan here because they did not respond to stimuli category condition\n",
    "                            \n",
    "                        else:\n",
    "                            output = process_event_data(raw_pup, smth_pup, sF, event, set_invalid_threshold, baseline_method = 'absolute') \n",
    "                            \n",
    "                        # slot in the timeseries to the correct rows in the dataframe \n",
    "                        index = int(behav_data.index[(behav_data['run'] == run) & (behav_data['trial'] == trial)][0]) \n",
    "                        behav_data.iat[index, behav_data.columns.get_loc(event['name'])] = output  # log the timeseries here\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        if resp_on == 'nan':\n",
    "                            behav_data.iat[index, behav_data.columns.get_loc(event['name'])] = 'nan'  # log nan here because they did not respond to stimuli category condition\n",
    "\n",
    "                        else:\n",
    "                            if (event['name'] == \"anticipation\") or (event['name'] == \"start_anticipation\") or (event['name'] == \"end_anticipation\"): \n",
    "                                \n",
    "                                # process the two pupil events of 0.5 s pupil (min and max)\n",
    "                                raw_trial_dat_min = raw_pup[int(event['resp_onset'])+(stim_dur*sF):int(event['resp_onset'])+(stim_dur*sF)+int(duration_max_anticipation*sF)]\n",
    "                                prop_invalid_min = (sum([int(x==-1) for x in raw_trial_dat_min])/len(raw_trial_dat_min))*100  # get proportion of invalid samles in event\n",
    "                                raw_trial_dat_max = raw_pup[int(event['tn_onset'])-int(duration_max_anticipation*sF):int(event['tn_onset'])]\n",
    "                                prop_invalid_max = (sum([int(x==-1) for x in raw_trial_dat_min])/len(raw_trial_dat_min))*100  # get proportion of invalid samles in event\n",
    "\n",
    "                                # only log anticipation event if both segments contain fewer than 50% invalid samples\n",
    "                                if (prop_invalid_min < set_invalid_threshold) and (prop_invalid_max < set_invalid_threshold):\n",
    "                                    min_ant = np.mean(smth_pup[int(event['resp_onset'])+(3*sF):int(event['resp_onset'])+(3*sF)+int(duration_max_anticipation*sF)])\n",
    "                                    max_ant = np.mean(smth_pup[int(event['tn_onset'])-int(duration_max_anticipation*sF):int(event['tn_onset'])])\n",
    "                                    anticipation_peak = max_ant-min_ant  # pupil change index\n",
    "\n",
    "                                    if (event['name'] == \"anticipation\"):\n",
    "                                        behav_data.iat[index, behav_data.columns.get_loc(event['name'])] = anticipation_peak  # log pupil change index here\n",
    "                                    elif (event['name'] == \"start_anticipation\"):\n",
    "                                        behav_data.iat[index, behav_data.columns.get_loc(event['name'])] = min_ant - event['baseline'] \n",
    "                                    elif (event['name'] == \"end_anticipation\"):\n",
    "                                        behav_data.iat[index, behav_data.columns.get_loc(event['name'])] = max_ant - event['baseline'] \n",
    "                                    invalid_anticipation.append(0)\n",
    "                                else:\n",
    "                                    invalid_anticipation.append(1)\n",
    "                                    behav_data.iat[index, behav_data.columns.get_loc(event['name'])] = 'nan'  # log nan here because there are too many invalid samples \n",
    "                                    #print(f'too many invalid sampels, anticipation for trial {trial}: nan')\n",
    "                            \n",
    "                            else: \n",
    "                                \n",
    "                                if event['name'] == 'choice_onset_event':\n",
    "                                    start = int(st_on) + (stim_dur*sF)\n",
    "                                    end = int(st_on) + (stim_dur*sF) + int(other_event_dur*sF)\n",
    "                                    \n",
    "                                     # log the proportion of invalid trials: \n",
    "                                    raw_trial_dat = raw_pup[start : end]\n",
    "                                    choice_prop_invalid = (sum([int(x==-1) for x in raw_trial_dat])/len(raw_trial_dat))*100  # get proportion of invalid samles in event\n",
    "                                    if choice_prop_invalid > set_invalid_threshold:\n",
    "                                        invalid_choice.append(1)\n",
    "                                    else:\n",
    "                                        invalid_choice.append(0)             \n",
    "                                                                           \n",
    "                                raw_trial_dat = raw_pup[start : end]\n",
    "                                prop_invalid = (sum([int(x==-1) for x in raw_trial_dat])/len(raw_trial_dat))*100  # get proportion of invalid samles in event\n",
    "                                \n",
    "                                # only log event if both segments contain fewer than 50% invalid samples\n",
    "                                if (prop_invalid < set_invalid_threshold) and (event['BL_prop_invalid'] < set_invalid_threshold):\n",
    "                                    mean_pup = np.mean(smth_pup[start : end]) - event['baseline']  \n",
    "                                    behav_data.iat[index, behav_data.columns.get_loc(event['name'])] = mean_pup  # log the pupil here\n",
    "\n",
    "                                else:\n",
    "                                    behav_data.iat[index, behav_data.columns.get_loc(event['name'])] = 'nan'  # log nan here because there are too many invalid samples \n",
    "                                    \n",
    "    # Compute a few averages                                \n",
    "    prop_choice = round(float(sum(invalid_choice) / len(invalid_choice)),2)\n",
    "    prop_BL = round(float(sum(invalid_BL) / len(invalid_BL)),2)\n",
    "    prob_anticipation = round(float(sum(invalid_anticipation) / len(invalid_anticipation)),2)\n",
    "    ave_Df['choice_event_invalid'] = [prop_choice]\n",
    "    ave_Df['BL_event_invalid'] = [prop_BL]\n",
    "    ave_Df['anticipation_invalid'] = [prob_anticipation]\n",
    "    \n",
    "    # sort the data \n",
    "    behav_data = behav_data.sort_values(by=['run'], ascending=True)\n",
    "    behav_data['trial_tot'] = [i+1 for i in range(len(behav_data['run']))]  # make total trial counter\n",
    "\n",
    "    # calculate some median scores\n",
    "    behav_data = behav_data.groupby('run').apply(apply_median_split, pupil='anticipation')\n",
    "    behav_data = behav_data.groupby('run').apply(apply_median_split, pupil='preStim_bl')\n",
    "    behav_data = behav_data.groupby('run').apply(apply_median_split, pupil='choice_onset_event')\n",
    "\n",
    "\n",
    "    # make two new columns: subject exclusions and trial exclusions\n",
    "    # --------------------------------------------------------------\n",
    "    ## Here: check whether any participants score chance level behaviour on precision mem test. (mean abs error < 75 degrees)\n",
    "    abs_minimum_error=[]\n",
    "    for i in range(len(behav_data['minimum_error'])):\n",
    "        if behav_data['minimum_error'][i] == 'NaN':\n",
    "            abs_minimum_error.append('NaN')\n",
    "        else: \n",
    "            abs_minimum_error.append(abs(behav_data['minimum_error'][i]))\n",
    "    behav_data['abs_minimum_error'] = abs_minimum_error \n",
    "    # calculate the mean of the absolute error\n",
    "    behav_data['abs_minimum_error'] = pd.to_numeric(behav_data['abs_minimum_error'], errors='coerce')\n",
    "    mean_abs = behav_data['abs_minimum_error'].mean(skipna=True)\n",
    "    # create the 'subj_excl' column\n",
    "    subj_excl_list = [0 if mean_abs <= 75 else 1] * len(behav_data)\n",
    "    # assign the list to a new column 'subj_excl'\n",
    "    behav_data['subj_excl'] = subj_excl_list\n",
    "    \n",
    "    \n",
    "    behav_data = pd.DataFrame(behav_data)\n",
    "    #behav_data.to_csv(save_fn, index=False)\n",
    "    return behav_data, ave_Df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude a further 2 (018, 003, 034)\n",
    "#exclude_cutoff = ['018', '003', '034']\n",
    "exclude_cutoff = ['034']\n",
    "finTrial_dat = finTrial_dat[~finTrial_dat['sub'].isin(exclude_cutoff)]\n",
    "len(set(finTrial_dat['sub']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pupil_drift(sub):\n",
    "    dat = add_pupilData(sub)\n",
    "    dat = dat.sort_values(['run', 'trial'],\n",
    "              ascending = [True, True])\n",
    "\n",
    "    fig, axes = plt.subplots(figsize=(30, 5), nrows=1, ncols=len(set(dat['run'])))\n",
    "    fig.suptitle(f\"sub-{sub}\", fontsize=16)\n",
    "    for i,run_nr in enumerate(set(dat['run'])):\n",
    "        run_data = dat.loc[dat['run'] == run_nr]\n",
    "        sns.barplot(data=run_data, x=\"trial\", y=\"preStim_bl\", ax=axes[i]).set(title = f'baseline pup run {run_nr}')\n",
    "\n",
    "def plot_reward_ts(data, event, baseline_dur, SEM=True, sF=40, ax = None,ylabel=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()  # Get the current axis if ax is not provided\n",
    "    if event == 'SP_timeseries':\n",
    "        win_start = baseline_dur\n",
    "        win_end = 6.0\n",
    "        corr_len = sF * (win_end+win_start)\n",
    "    elif event == 'FB_timeseries':\n",
    "        win_start = baseline_dur\n",
    "        win_end = 3.0\n",
    "        corr_len = sF * (win_end+win_start)\n",
    "    elif event == 'resp_onset_event':\n",
    "        win_start = 0\n",
    "        win_end = 6.0\n",
    "        corr_len = sF * (win_end+win_start)\n",
    "        \n",
    "    x = np.array(range(1,int((win_end--win_start)*sF)+1))\n",
    "\n",
    "    dat = data[data[event].apply(len) == corr_len] # if a trial has NAN for timeseries, it will not be included (i.e., doesn't meet inclusion criteria)\n",
    "    \n",
    "    # exclude trials which were wrongly classified at the start of the trial \n",
    "    excl_trial = dat['cat_corr']\n",
    "    excl_bool_mask = np.where(np.array(excl_trial) == 0, False, True)\n",
    "    dat = dat[excl_bool_mask]\n",
    "    \n",
    "    # collapse across subs\n",
    "    grouped = dat.groupby(['sub', 'trial_type'])[event].apply(lambda x: np.nanmean(x.tolist(), axis=0))\n",
    "    df = grouped.reset_index()\n",
    "    df.columns = ['sub', 'trial_type', event]\n",
    "\n",
    "    # calculate mean and SEM\n",
    "    rew = df[df['trial_type'] == 'csp'][event]\n",
    "    neu = df[df['trial_type'] == 'csm'][event]\n",
    "    mean_rew = np.nanmean(rew.to_list(), axis=0)\n",
    "    mean_neu = np.nanmean(neu.to_list(), axis=0)\n",
    "\n",
    "    # Calculate the derivative or rate of change\n",
    "    grad_neu = np.gradient(mean_neu, range(0, len(mean_neu)))\n",
    "\n",
    "    # Find the maximum rate of change (steepness) rising to the peak\n",
    "    peak_steepness_neu = np.max(grad_neu)\n",
    "    print(\"Neutral: Steepness of slope rising to the peak:\", peak_steepness_neu)\n",
    "    \n",
    "    rew_SEM = scipy.stats.sem(rew.to_list(), axis=0, nan_policy= 'omit')\n",
    "    rew_SEM1 = mean_rew - rew_SEM \n",
    "    rew_SEM2 = mean_rew + rew_SEM\n",
    "\n",
    "    neu_SEM = scipy.stats.sem(neu.to_list(), axis=0, nan_policy= 'omit')\n",
    "    neu_SEM1 = mean_neu - neu_SEM \n",
    "    neu_SEM2 = mean_neu + neu_SEM\n",
    "\n",
    "    # Statistical test: point-wise two-sided t-test\n",
    "    stat, p_val_uncor = st.permtest_rel(rew.to_list(), neu.to_list(), axis=0)\n",
    "    p_val_cor=sm.stats.fdrcorrection(p_val_uncor, alpha=0.05)\n",
    "    sig_point=np.where(p_val_cor[0]==True)\n",
    "    \n",
    "    \n",
    "    # plot data\n",
    "    ax.plot(mean_rew, label='reward', color = 'green') \n",
    "    ax.plot(mean_neu, label='neutral', color =  'grey') \n",
    "    \n",
    "    if SEM: \n",
    "        ax.fill_between(x, rew_SEM1,rew_SEM2, alpha = 0.4,color = 'green')\n",
    "        ax.fill_between(x, neu_SEM1,neu_SEM2, alpha = 0.4, color =  'grey')\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(np.arange(sF*baseline_dur, len(rew[1]), sF*1).tolist(),np.arange(0, round(win_end + baseline_dur), 1).tolist())\n",
    "    ax.legend().remove()\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(\"Pupil size [mm]\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "    if (event == 'SP_timeseries'):\n",
    "        ax.hlines(y=max(rew_SEM2+0.05),xmin = 0.2*sF, xmax = 3.2*sF, linestyles = 'dashed') \n",
    "        ax.vlines(ymax=max(rew_SEM2+0.05), ymin=min(neu_SEM2-0.05), x=sF*3.2, colors='blue', linewidth=1, alpha=0.2)\n",
    "        ax.vlines(ymax=max(rew_SEM2+0.05), ymin=min(neu_SEM2-0.05), x=sF*3.4, colors='blue', linewidth=1, alpha=0.2)\n",
    "    elif event == 'FB_timeseries':\n",
    "        ax.hlines(y=max(rew_SEM2+0.05),xmin =  0.2*sF, xmax = 0.7*sF, linestyles = 'dashed') \n",
    "    elif event == 'resp_onset_event':\n",
    "        ax.hlines(y=max(rew_SEM2+0.05),xmin =  0.5*sF, xmax = 3.0*sF, linestyles = 'dashed') \n",
    "    if len(sig_point[0]) > 0:\n",
    "        ax.hlines(y=max(rew_SEM2+0.05),xmin = sig_point[0][0], xmax = sig_point[0][-1], color = 'blue') # sig line \n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)    \n",
    "        \n",
    "    # save the plot\n",
    "    #plt.savefig(f'../../stats/figures/pupil/timeseries_plots/{event}_split_reward.pdf')\n",
    "    #plt.savefig(f'../../stats/figures/pupil/timeseries_plots/{event}_split_reward.png', dpi=300)\n",
    "\n",
    "\n",
    "def plot_fb_ts(data, event, baseline_dur, sF=40,ax = None, ylabel=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()  # Get the current axis if ax is not provided\n",
    "    if event == 'SP_timeseries':\n",
    "        win_start = baseline_dur\n",
    "        win_end = 6.0 \n",
    "        corr_len = sF * (win_end+win_start)\n",
    "        \n",
    "    elif event == 'FB_timeseries':\n",
    "        win_start = baseline_dur\n",
    "        win_end = 3.0\n",
    "        corr_len = sF * (win_end+win_start)\n",
    "        \n",
    "    x = np.array(range(1,int((win_end--win_start)*sF)+1))\n",
    "\n",
    "    dat = data[data[event].apply(len) == corr_len]\n",
    "    # exclude trials which were wrongly classified at the start of the trial \n",
    "    excl_trial = dat['cat_corr']\n",
    "    excl_bool_mask = np.where(np.array(excl_trial) == 0, False, True)\n",
    "    dat = dat[excl_bool_mask]\n",
    "    \n",
    "    \n",
    "    # collapse across subs\n",
    "    grouped = dat.groupby(['sub', 'feedback_type'])[event].apply(lambda x: np.nanmean(x.tolist(), axis=0))\n",
    "    df = grouped.reset_index()\n",
    "    df.columns = ['sub', 'feedback_type', event]\n",
    "\n",
    "    # calculate mean and SEM\n",
    "    pos = df[df['feedback_type'] == 'pos_feedback_onset'][event]\n",
    "    neg = df[df['feedback_type'] == 'neg_feedback_onset'][event]\n",
    "    neu = df[df['feedback_type'] == 'neu_feedback_onset'][event]\n",
    "    mean_pos = np.nanmean(pos.to_list(), axis=0)\n",
    "    mean_neg = np.nanmean(neg.to_list(), axis=0)\n",
    "    mean_neu = np.nanmean(neu.to_list(), axis=0)\n",
    "\n",
    "    pos_SEM = scipy.stats.sem(pos.to_list(), axis=0, nan_policy= 'omit')\n",
    "    pos_SEM1 = mean_pos - pos_SEM \n",
    "    pos_SEM2 = mean_pos + pos_SEM\n",
    "\n",
    "    neg_SEM = scipy.stats.sem(neg.to_list(), axis=0, nan_policy= 'omit')\n",
    "    neg_SEM1 = mean_neg - neg_SEM \n",
    "    neg_SEM2 = mean_neg + neg_SEM\n",
    "    \n",
    "    neu_SEM = scipy.stats.sem(neu.to_list(), axis=0, nan_policy= 'omit')\n",
    "    neu_SEM1 = mean_neu - neu_SEM \n",
    "    neu_SEM2 = mean_neu + neu_SEM\n",
    "\n",
    "    # plot data\n",
    "    ax.plot(mean_pos, label='pos', color = 'green') \n",
    "    ax.plot(mean_neg, label='neg', color =  'orange') \n",
    "    ax.plot(mean_neu, label='neu', color =  'grey') \n",
    "    \n",
    "    ax.fill_between(x, pos_SEM1,pos_SEM2, alpha = 0.4,color = 'green')\n",
    "    ax.fill_between(x, neg_SEM1,neg_SEM2, alpha = 0.4, color =  'orange')\n",
    "    ax.fill_between(x, neu_SEM1,neu_SEM2, alpha = 0.4, color =  'grey')\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(np.arange(20, corr_len, sF*1).tolist(),np.arange(0, round(win_end + 0.5), 1).tolist())\n",
    "    ax.legend().remove()\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(\"Pupil size [mm]\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "    if event == 'SP_timeseries':\n",
    "        ax.hlines(y=max(neg_SEM2+0.05),xmin = 20, xmax = 3.5*40, linestyles = 'dashed') # sharon sig line\n",
    "    elif event == 'FB_timeseries':\n",
    "        ax.hlines(y=max(neg_SEM2+0.05),xmin = 20, xmax = 1*40, linestyles = 'dashed') # sharon sig line\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # save the plot\n",
    "   # plt.savefig(f'../../stats/figures/pupil/timeseries_plots/{event}_split_feedback.pdf')\n",
    "    #plt.savefig(f'../../stats/figures/pupil/timeseries_plots/{event}_split_feedback.png', dpi=300)\n",
    "    \n",
    "def convert_pvalue_to_asterisks(pvalue):\n",
    "    if pvalue <= 0.0001:\n",
    "        return \"****\"\n",
    "    elif pvalue <= 0.001:\n",
    "        return \"***\"\n",
    "    elif pvalue <= 0.01:\n",
    "        return \"**\"\n",
    "    elif pvalue <= 0.05:\n",
    "        return \"*\"\n",
    "    return \"ns\"\n",
    "\n",
    "def plot_ave_pup(data, event, ax = None, ylabel=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()  # Get the current axis if ax is not provided\n",
    "    if event == 'SP_peak_response':\n",
    "        title = 'stimulus presentation'\n",
    "    elif event == 'FB_peak_response':\n",
    "        title = 'feedback presentation'\n",
    "    elif event == 'anticipation':\n",
    "        title = 'reward anticipation'\n",
    "    elif event == 'start_anticipation':\n",
    "        title = 'early reward anticipation'\n",
    "    elif event == 'end_anticipation':\n",
    "        title = 'late reward anticipation'\n",
    "    elif event == 'preStim_bl':\n",
    "        title = 'pre-stimulus baseline (0.5s)'\n",
    "    elif event == 'choice_onset_event':\n",
    "        title = 'pupil at choice onset (0.5s)'\n",
    "    elif event == 'resp_onset_event':\n",
    "        title = 'pupil at mouse response (0.5s)'\n",
    "        \n",
    "    # exclude trials which were wrongly classified at the start of the trial \n",
    "    excl_trial = data['cat_corr']\n",
    "    excl_bool_mask = np.where(np.array(excl_trial) == 0, False, True)\n",
    "    data = data[excl_bool_mask]    \n",
    "    \n",
    "    grouped = data.groupby(['sub', 'trial_type'])[event].apply(lambda x: np.nanmean(x.tolist(), axis=0))\n",
    "    df = grouped.reset_index()\n",
    "    df.columns = ['sub', 'trial_type', event]\n",
    "    \n",
    "    # change name \n",
    "    df.loc[df['trial_type'] == 'csp', 'trial_type'] = 'reward'\n",
    "    df.loc[df['trial_type'] == 'csm', 'trial_type'] = 'neutral'\n",
    "    \n",
    "    # Group by 'trial_type' and calculate mean\n",
    "    means = df.groupby('trial_type')[event].mean()\n",
    "    # Group by 'trial_type' and calculate standard error\n",
    "    standard_errors = df.groupby('trial_type')[event].apply(sem)\n",
    "\n",
    "    # Print the mean and standard error\n",
    "    print('Mean:')\n",
    "    print(means)\n",
    "    print('\\nStandard Error:')\n",
    "    print(standard_errors)\n",
    "    \n",
    "    # perform paired t-test\n",
    "    stat, pvalue = scipy.stats.ttest_rel(df[df['trial_type']==f'reward'][event],\n",
    "    df[df['trial_type']==f'neutral'][event])\n",
    "    sig_lev = convert_pvalue_to_asterisks(pvalue)\n",
    "#     if pvalue < 0.5:\n",
    "#         print(f'p = {round(pvalue,3)}')\n",
    "    print(f'p = {pvalue,3}')\n",
    "    if (event == 'SP_peak_response') or (event == 'FB_peak_response'):\n",
    "        plt_title = f'peak pupil response: {title}'\n",
    "    else: \n",
    "        plt_title = title\n",
    "    \n",
    "    sns.set(rc={'figure.figsize':(2,4)}) \n",
    "    sns.set_style('white')\n",
    "    sns.barplot(data=df, x=\"trial_type\", y=event,capsize=.1, edgecolor=\".1\", palette=['grey', 'green'], alpha=.8,ax=ax)\n",
    "    sns.swarmplot(data=df, x=\"trial_type\", y=event, color=\"0\", alpha=.35, ax=ax)\n",
    "\n",
    "    ax.text(0.5,max(df[event]),sig_lev,horizontalalignment='center', verticalalignment='top', fontsize = 13)\n",
    "    \n",
    "    if ylabel:\n",
    "        if event == 'preStim_bl':\n",
    "            ax.set_ylabel(\"Pupil size [mm]\")\n",
    "        elif event == 'anticipation':\n",
    "            ax.set_ylabel(\"Change in pupil size [mm]\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"Pupil size [mm]\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # save the plot\n",
    "    #plt.savefig(f'../../stats/figures/pupil/average_plots/{event}_split_reward.pdf')\n",
    "    #plt.savefig(f'../../stats/figures/pupil/average_plots/{event}_split_reward.png', bbox_inches='tight', dpi = 300)\n",
    "    \n",
    "    \n",
    "def plot_trial_pup(data, event):\n",
    "    \n",
    "    if event == 'SP_peak_response':\n",
    "        title = 'stimulus presentation'\n",
    "    elif event == 'FB_peak_response':\n",
    "        title = 'feedback presentation'\n",
    "    elif event == 'anticipation':\n",
    "        title = 'reward anticipation'\n",
    "    elif event == 'preStim_bl':\n",
    "        title = 'pre-stimulus baseline'\n",
    "\n",
    "   \n",
    "    # change name \n",
    "    data.loc[data['trial_type'] == 'csp', 'trial_type'] = 'reward'\n",
    "    data.loc[data['trial_type'] == 'csm', 'trial_type'] = 'neutral'\n",
    "\n",
    "    sns.stripplot(data=trial_dat, x=\"trial_type\", y='anticipation',  alpha=.35, size=2, hue = 'sub').set(title = f'trial-level peak pupil response: {title}')\n",
    "    \n",
    "    plt.ylabel(\"Pupil size [% change]\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "def plot_trial_level_regression_conditions(data, event_pup, event_behav):\n",
    "    \n",
    "    if event_pup == 'preStim_bl_zscore':\n",
    "        title_y = 'Pupil size [z-score]'\n",
    "    else:\n",
    "        title_y = 'Pupil size [% change]'\n",
    "        \n",
    "    if event_behav == 'minimum_error':\n",
    "        title_x  = 'Minimum Error (abs)'\n",
    "    elif event_behav == 'TN_rt':\n",
    "        title_x  = 'Reaction time (ms)'\n",
    "    \n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fig.suptitle(f\"{event}: trial-level data\", fontsize=16)\n",
    "    \n",
    "    # Filter the data by the 'reward' column\n",
    "    reward_conditions = ['csp', 'csm']\n",
    "    colours = ['green', 'grey']\n",
    "    for i, condition in enumerate(reward_conditions):\n",
    "        trial_dat_full = data.dropna(subset=[event, event_behav])\n",
    "        \n",
    "        # exclude trials which were wrongly classified at the start of the trial \n",
    "#         excl_trial = trial_dat_full['cat_corr']\n",
    "#         excl_bool_mask = np.where(np.array(excl_trial) == 0, False, True)\n",
    "#         trial_dat_full = trial_dat_full[excl_bool_mask]\n",
    "        \n",
    "        trial_dat_condition = trial_dat_full[trial_dat_full['trial_type'] == condition]\n",
    "        trial_dat_condition = trial_dat_condition.dropna(subset=[event, event_behav])\n",
    "        trial_dat_condition = trial_dat_condition[trial_dat_condition[event_behav] != 'NaN']\n",
    "        trial_dat_condition = trial_dat_condition[trial_dat_condition[event] != 'NaN']\n",
    "        \n",
    "        if event_behav == 'minimum_error':\n",
    "            x_condition = trial_dat_condition[event_behav].astype(float).abs()\n",
    "        else: \n",
    "            x_condition = trial_dat_condition[event_behav].astype(float)\n",
    "        y_condition = trial_dat_condition[event].astype(float)\n",
    "        num_nans = np.isnan(x_condition).sum()\n",
    "\n",
    "        # Carry out the regression analysis with the non-NaN values of X and Y\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x_condition, y_condition)\n",
    "        #print(f'{condition} - regression slope R = {round(r_value, 3)} and p = {round(p_value, 10)}')\n",
    "    \n",
    "        # Compute the regression line\n",
    "        reg_line = slope*x_condition + intercept\n",
    "\n",
    "        # Plot the scatter plot with the regression line in each subplot\n",
    "        ax = [ax1, ax2][i]\n",
    "        ax.scatter(x_condition, y_condition, s=3, color=colours[i])\n",
    "        ax.plot(x_condition, reg_line, color=colours[i], linewidth=2, \n",
    "                label=f'y = {slope:.2f}x + {intercept:.2f}\\n r = {r_value:.2f}')\n",
    "        ax.set_xlabel(title_x)\n",
    "        ax.set_ylabel(title_y)\n",
    "        ax.set_title(condition.capitalize())\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_trial_level_regression_collapsed(data, event_pup, event_behav):\n",
    "    \n",
    "    if event_pup == 'preStim_bl_zscore':\n",
    "        title_y = 'Pupil size [z-score]'\n",
    "    else:\n",
    "        title_y = 'Pupil size [% change]'\n",
    "        \n",
    "    if event_behav == 'minimum_error':\n",
    "        title_x  = 'Minimum Error (abs)'\n",
    "    elif event_behav == 'TN_rt':\n",
    "        title_x  = 'Reaction time (ms)'\n",
    "    \n",
    "\n",
    "    trial_dat_full = data.dropna(subset=[event, event_behav])\n",
    "    trial_dat_full = trial_dat_full.dropna(subset=[event, event_behav])\n",
    "    trial_dat_full = trial_dat_full[trial_dat_full[event_behav] != 'NaN']\n",
    "    trial_dat_full = trial_dat_full[trial_dat_full[event] != 'NaN']\n",
    "\n",
    "    if event_behav == 'minimum_error':\n",
    "        x_condition = trial_dat_full[event_behav].astype(float).abs()\n",
    "    else: \n",
    "        x_condition = trial_dat_full[event_behav].astype(float)\n",
    "    y_condition = trial_dat_full[event].astype(float)\n",
    "    num_nans = np.isnan(x_condition).sum()\n",
    "\n",
    "    # Carry out the regression analysis with the non-NaN values of X and Y\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x_condition, y_condition)\n",
    "    #print(f'{condition} - regression slope R = {round(r_value, 3)} and p = {round(p_value, 10)}')\n",
    "\n",
    "    # Compute the regression line\n",
    "    reg_line = slope*x_condition + intercept\n",
    "\n",
    "    # Plot the scatter plot with the regression line in each subplot\n",
    "    plt.scatter(x_condition, y_condition, s=3)\n",
    "    plt.plot(x_condition, reg_line, linewidth=2, \n",
    "            label=f'y = {slope:.2f}x + {intercept:.2f}\\n r = {r_value:.2f}')\n",
    "    plt.xlabel(title_x)\n",
    "    plt.ylabel(title_y)\n",
    "    plt.legend()\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def subject_level_regression_conditions(data, event):\n",
    "\n",
    "    if event == 'preStim_bl_zscore':\n",
    "        title_plot = 'Pupil size [z-score]'\n",
    "    else:\n",
    "        title_plot = 'Pupil size [% change]'\n",
    "\n",
    "        # remove nans\n",
    "    data = data[data['minimum_error'] != 'NaN']\n",
    "    data = data[data[event] != 'NaN']\n",
    "\n",
    "    data['minimum_error_abs'] = data['minimum_error'].abs()\n",
    "    data['minimum_error_abs'] = data['minimum_error_abs'].astype(float)\n",
    "\n",
    "    averaged = data.groupby(['sub', 'trial_type'])[event, 'minimum_error_abs'].mean().reset_index()\n",
    "\n",
    "    # # Separate the data based on condition\n",
    "    reward = averaged[averaged['trial_type'] == 'csp']\n",
    "    neutral = averaged[averaged['trial_type'] == 'csm']\n",
    "\n",
    "    # get statistics \n",
    "    slope_rew, intercept_rew, r_value_rew, p_value_rew, std_err_rew = linregress(reward['minimum_error_abs'], reward[event])\n",
    "    # # Compute the regression line\n",
    "    reg_line_rew = slope_rew*reward['minimum_error_abs'] + intercept_rew\n",
    "    print(f'p-val for reward regression: {p_value_rew}')\n",
    "    # # get statistics \n",
    "    slope_neu, intercept_neu, r_value_neu, p_value_neu, std_err_neu = linregress(neutral['minimum_error_abs'], neutral[event])\n",
    "    # Compute the regression line\n",
    "    reg_line_neu = slope_neu*neutral['minimum_error_abs'] + intercept_neu\n",
    "    print(f'p-val for neutral regression: {p_value_neu}')\n",
    "    # # make regression plot \n",
    "    plt.scatter(reward['minimum_error_abs'], reward[event], color = 'green', s=20)\n",
    "    plt.plot(reward['minimum_error_abs'], reg_line_rew, color='green', linewidth=2,\n",
    "                     label=f'y = {slope_rew:.2f}x + {intercept_rew:.2f}\\n r = {r_value_rew:.2f}')\n",
    "\n",
    "    plt.scatter(neutral['minimum_error_abs'], neutral[event], color = 'grey', s=20)\n",
    "    plt.plot(neutral['minimum_error_abs'], reg_line_neu, color='grey', linewidth=2,\n",
    "                     label=f'y = {slope_neu:.2f}x + {intercept_neu:.2f}\\n r = {r_value_neu:.2f}')\n",
    "\n",
    "\n",
    "    # Add axis labels and legend\n",
    "    plt.xlabel('Minimum Error (abs)')\n",
    "    plt.ylabel(title_plot)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.title(f\"{event}: participant-level average\")\n",
    "    \n",
    "def plot_start_end_ant(data, ax = None, ylabel=True):\n",
    "    data = finTrial_dat\n",
    "    title = 'anticipatory pupil'\n",
    "\n",
    "    # exclude trials which were wrongly classified at the start of the trial \n",
    "    excl_trial = data['cat_corr']\n",
    "    excl_bool_mask = np.where(np.array(excl_trial) == 0, False, True)\n",
    "    data = data[excl_bool_mask]    \n",
    "\n",
    "    #grouped = data.groupby(['sub'])['start_anticipation'].apply(lambda x: np.nanmean(x.tolist(), axis=0))\n",
    "    grouped = data.groupby(['sub'])['start_anticipation', 'end_anticipation'].mean()\n",
    "    df = grouped.reset_index()\n",
    "    df = pd.melt(df, id_vars='sub', value_vars=['start_anticipation', 'end_anticipation'])\n",
    "    df.columns = ['sub', 'event', 'pupil_size']\n",
    "    palette = sns.color_palette(['grey'], len(df['sub'].unique()))\n",
    "    fig = sns.set(rc={'figure.figsize':(2,4)}) \n",
    "    fig = sns.set_style('white')\n",
    "    #sns.barplot(data=df, x=\"event\", y='pupil_size',capsize=.1, edgecolor=\".1\", palette=['grey', 'green'], alpha=.8)\n",
    "    fig = sns.swarmplot(data=df, x=\"event\", y='pupil_size', color=\"0\", alpha=.35, size = 5)\n",
    "    fig = sns.lineplot(x=\"event\", y=\"pupil_size\", hue='sub', sort=False, palette =palette, data=df,legend=False)\n",
    "    fig.set_ylabel(\"Pupil size [mm]\")\n",
    "\n",
    "    fig.spines['top'].set_visible(False)\n",
    "    fig.spines['right'].set_visible(False)\n",
    "\n",
    "    # save the plot\n",
    "    #plt.savefig(f'../../stats/figures/pupil/average_plots/{event}_split_reward.pdf')\n",
    "    #plt.savefig(f'../../stats/figures/pupil/average_plots/{event}_split_reward.png', bbox_inches='tight', dpi = 300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_behaviour_checks(data, event,ax = None, ylabel= None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()  # Get the current axis if ax is not provided\n",
    "    if event == 'cat_corr':\n",
    "        title = 'category classification'\n",
    "    elif event == 'TN_corr':\n",
    "        title = 'target number classification'\n",
    "    elif event == 'TN_rt':\n",
    "        title = 'target number reaction time'\n",
    " \n",
    "    data[event] = pd.to_numeric(data[event], errors='coerce')\n",
    "    grouped = data.groupby(['sub', 'trial_type'])[event].apply(lambda x: np.nanmean(x.tolist(), axis=0))\n",
    "    df = grouped.reset_index()\n",
    "    \n",
    "    df.columns = ['sub', 'trial_type', event]\n",
    "    \n",
    "    # change name \n",
    "    df.loc[df['trial_type'] == 'csp', 'trial_type'] = 'reward'\n",
    "    df.loc[df['trial_type'] == 'csm', 'trial_type'] = 'neutral'\n",
    "    \n",
    "# Group by 'trial_type' and calculate mean\n",
    "    means = df.groupby('trial_type')[event].mean()\n",
    "\n",
    "    # Group by 'trial_type' and calculate standard error\n",
    "    standard_errors = df.groupby('trial_type')[event].apply(sem)\n",
    "\n",
    "    # Print the mean and standard error\n",
    "    print('Mean:')\n",
    "    print(means)\n",
    "    print('\\nStandard Error:')\n",
    "    print(standard_errors)\n",
    "\n",
    "    \n",
    "    # perform paired t-test\n",
    "    stat, pvalue = scipy.stats.ttest_rel(df[df['trial_type']==f'reward'][event],\n",
    "    df[df['trial_type']==f'neutral'][event])\n",
    "    sig_lev = convert_pvalue_to_asterisks(pvalue)\n",
    "    #if pvalue < 0.5:\n",
    "    print(f'{event} p = {pvalue}')\n",
    "        \n",
    "    sns.set(rc={'figure.figsize':(2,4)}) \n",
    "    sns.set_style('white')\n",
    "    sns.barplot(data=df, x=\"trial_type\", y=event,capsize=.1, edgecolor=\".1\", palette=['grey', 'green'],alpha=.8,ax=ax)\n",
    "    sns.swarmplot(data=df, x=\"trial_type\", y=event, color=\"0\", alpha=.35, ax=ax)\n",
    "\n",
    "    if (event == 'TN_rt'):\n",
    "        ax.text(0.5,max(df[event])+50,sig_lev,horizontalalignment='center', verticalalignment='top', fontsize = 13)\n",
    "    else:\n",
    "        ax.text(0.5,max(df[event])+0.1,sig_lev,horizontalalignment='center', verticalalignment='top', fontsize = 13)\n",
    "    #ax.set_ylim(top=1.1)\n",
    "    if ylabel:\n",
    "        if (event == 'cat_corr') or (event == 'TN_corr'):\n",
    "            ax.set_ylabel(\"Proportion correct\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"Reaction time (ms)\")\n",
    "    else: \n",
    "        ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    #plt.tight_layout()\n",
    "    # save the plot\n",
    "    #plt.savefig(f'../../stats/figures/behaviour/{event}_split_reward.pdf')\n",
    "    #plt.savefig(f'../../stats/figures/behaviour/{event}_split_reward.png',bbox_inches='tight', dpi = 300)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run the preprocessing here:\n",
    "'''\n",
    "\n",
    "hrbp_path = 'C:\\\\Users\\\\lloydb\\\\surfdrive\\\\ExperimentData\\\\HRBP_MP'\n",
    "file_list = glob.glob(os.path.join(hrbp_path, 'raw_data', 'data', 'HRBP*'))\n",
    "subjectlist = [os.path.split(file)[1][-3:] for file in file_list]\n",
    "\n",
    "trial_dat = pd.DataFrame()\n",
    "ave_dat = pd.DataFrame()\n",
    "for sub in subjectlist:\n",
    "    print(f'running subject: {sub}')\n",
    "    get_precision_data(sub)\n",
    "    sub_dat, ave_sub = add_pupilData(sub)\n",
    "    \n",
    "    trial_dat = trial_dat.append(sub_dat, ignore_index=True)\n",
    "    ave_dat = ave_dat.append(ave_sub, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the exclusion checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## excl the chance-level behaviour people! (12 excluded) defined as a mean absolute error of \\> 75 degrees (where chance = 90) in location response\n",
    "finTrial_dat = trial_dat[trial_dat['subj_excl'] == 0]\n",
    "set(finTrial_dat['sub'])\n",
    "len(set(finTrial_dat['sub']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sub-034 is the additional sub who had to be removed after removing incorrectly classified trials (Do you expect a reward on this trial?) -> too few remaining trials for model fitting\n",
    "##### Note: after revisions, we ran an outlier detection analysis, this flagged subs 018 and 003 --> all analyses can be checked by removing these two here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude a further 2 (018, 003, 034)\n",
    "#exclude_cutoff = ['018', '003', '034']\n",
    "exclude_cutoff = ['034']\n",
    "finTrial_dat = finTrial_dat[~finTrial_dat['sub'].isin(exclude_cutoff)]\n",
    "len(set(finTrial_dat['sub']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make figure plots  (other plots made in R)\n",
    "### figure 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(1, 4, figsize=(14,3))\n",
    "plot_ave_pup(data=finTrial_dat, event='preStim_bl', ax = ax1, ylabel=True)\n",
    "plot_ave_pup(data=finTrial_dat, event='SP_peak_response', ax = ax2, ylabel=True)\n",
    "plot_ave_pup(data=finTrial_dat, event='choice_onset_event', ax = ax3, ylabel=False)\n",
    "plot_ave_pup(data=finTrial_dat, event='anticipation', ax = ax4, ylabel=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../../stats/figures/Figure2_pupBarplots.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(14,3))\n",
    "plot_reward_ts(data=finTrial_dat, event='SP_timeseries', baseline_dur=baseline_dur, ax = ax1,ylabel=True)\n",
    "plot_reward_ts(data=finTrial_dat, event='FB_timeseries', baseline_dur=baseline_dur, ax = ax2, ylabel=False)\n",
    "plot_fb_ts(data=trial_dat,  event='FB_timeseries', baseline_dur=baseline_dur,ax = ax3,ylabel=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../../stats/figures/Figure2_TSplots.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(6,3))\n",
    "\n",
    "plot_behaviour_checks(finTrial_dat, event = 'cat_corr', ax = ax1, ylabel= True)\n",
    "plot_behaviour_checks(finTrial_dat, event = 'TN_corr', ax = ax2, ylabel= False)\n",
    "plot_behaviour_checks(finTrial_dat, event = 'TN_rt', ax = ax3, ylabel= True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../../stats/figures/Figure2A.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(7,3)) #plt.figure(figsize=(3, 2.8))\n",
    "sub_regression_plot(finTrial_dat, 'preStim_bl', ax = ax1, ylabel = True)\n",
    "sub_regression_plot(finTrial_dat, 'choice_onset_event', ax = ax2, ylabel = False)\n",
    "sub_regression_plot(finTrial_dat, 'anticipation', ax = ax3, ylabel = False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../../stats/figures/Fig2B_new_regression_plots.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print some descriptive information! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_subs = set(finTrial_dat['sub'])\n",
    "ave_dat_fin = ave_dat[ave_dat['sub'].isin(final_subs)]\n",
    "\n",
    "means = ave_dat_fin['prop_rewarded'].mean()\n",
    "standard_errors = ave_dat_fin['prop_rewarded'].sem()\n",
    "\n",
    "# Print the mean and standard error\n",
    "print('Mean prop_rewarded:')\n",
    "print(round(means,2))\n",
    "print('Standard Error prop_rewarded:')\n",
    "print(round(standard_errors,2))\n",
    "\n",
    "means = ave_dat_fin['choice_event_invalid'].mean()\n",
    "standard_errors = ave_dat_fin['choice_event_invalid'].sem()\n",
    "\n",
    "# Print the mean and standard error\n",
    "print('\\nMean choice_event_invalid:')\n",
    "print(round(means,2))\n",
    "print('Standard Error choice_event_invalid:')\n",
    "print(round(standard_errors,2))\n",
    "\n",
    "means = ave_dat_fin['BL_event_invalid'].mean()\n",
    "standard_errors = ave_dat_fin['BL_event_invalid'].sem()\n",
    "\n",
    "# Print the mean and standard error\n",
    "print('\\nMean BL_event_invalid:')\n",
    "print(round(means,2))\n",
    "print('Standard Error BL_event_invalid:')\n",
    "print(round(standard_errors,2))\n",
    "\n",
    "means = ave_dat_fin['anticipation_invalid'].mean()\n",
    "standard_errors = ave_dat_fin['anticipation_invalid'].sem()\n",
    "\n",
    "# Print the mean and standard error\n",
    "print('\\nMean anticipation_invalid:')\n",
    "print(round(means,2))\n",
    "print('Standard Error anticipation_invalid:')\n",
    "print(round(standard_errors,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data here for further analysis in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv \n",
    "finTrial_dat.to_csv(f'{hrbp_path}\\\\stats\\\\1_preprocessed\\\\group_data\\\\HRBP_trial_dataframe.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all columns except timeseries to save data\n",
    "non_timeseries_dat = trial_dat.loc[:, trial_dat.columns != 'SP_timeseries']\n",
    "non_timeseries_dat = non_timeseries_dat.loc[:, non_timeseries_dat.columns != 'FB_timeseries']\n",
    "\n",
    "# save csv \n",
    "non_timeseries_dat.to_csv(f'{hrbp_path}\\\\stats\\\\1_preprocessed\\\\group_data\\\\behav_pupil_trial_data.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
